-----

# LLMQA

## Overview

LLMQA is a pipeline for Open-Domain Question Answering that utilizes Large Language Models (LLMs) for expansion generation and document reranking. This project supports datasets like WebQSP and Natural Questions (NQ).

## Dataset Preparation (WebQSP)

To ensure compatibility with the LLMQA pipeline, the original WebQSP dataset must be converted into a specific format.

### 1\. Conversion Script

We provide a script `Convert_webQSP.py` to convert the original `WebQSP.test.json` into the format required by our system.

**Command:**

```bash
python Convert_webQSP.py
```

### 2\. Dataset Files

After running the conversion or setting up your `input_data/webq` directory, you will have the following files:

  * **`test_full_new.json`**: This is the **original test set** converted into the suitable format for the pipeline. It is generated by the `Convert_webQSP.py` script.
  * **`test.json`**: This file contains a subset/sample of questions from `test_full_new.json`. It is used for smaller-scale testing.

## Running the Pipeline

The main entry point for the pipeline is `run.py`. This script handles the following 4 stages sequentially:

1.  Expansion Generation
2.  Expansion Evaluation
3.  Document Reranking
4.  Reranking Evaluation

### API Configuration

Ensure you have access to the required LLM engines (e.g., via Groq). You may need to export your API keys in your environment variables before running the scripts:

```bash
export GROQ_API_KEY="your_api_key_here"
```

### Usage Examples

#### 1\. WebQSP (Test Run)

To run the pipeline on the WebQSP dataset with the Deepseek engine:

```bash
python run.py --batch_size 1 --rerank_n 8 --engine deepseek-r1-distill-llama-70b
```

#### 2\. WebQSP (Specific Input/Output Directories)

To specify exact input and output directories for the WebQ test set:

```bash
python run.py --batch_size 1 --rerank_n 8 --engine deepseek-r1-distill-llama-70b --input_dir input_data --output_dir output_results --dataset webq --split test
```

#### 3\. Natural Questions (NQ)

To run the pipeline on the Natural Questions (NQ) dataset with specific sampling parameters:

```bash
python run.py --input_dir input_data --output_dir output_results --dataset nq --split test --engine deepseek-r1-distill-qwen-32b --batch_size 3 --temperature 0.6 --rerank_n 8 --top_p 0.95
```

#### 4\. Quick Debug Run

For a very quick test with a smaller rerank count:

```bash
python run.py --batch_size 1 --rerank_n 2 --engine deepseek-r1-distill-llama-70b
```

## Evaluation

After the pipeline completes, the results are stored in the `output_result` (or specified output) directory. You can evaluate the recall metrics using the provided utility script.

### Reranking Evaluation

To calculate the Recall metrics (Top-2, Top-4, Top-8) for the WebQSP results:

```bash
python utils/evaluate_recall.py --results_file output_result/webq/test/3-reranking_evaluation.jsonl
```

*Note: Ensure the `results_file` path points to the `3-reranking_evaluation.jsonl` file generated during the run step.*

## Outputs

The `run.py` script generates four files in the output directory (e.g., `output_result/webq/test/`):

1.  `0-expansion_generation.jsonl`: Raw expansions generated by the LLM.
2.  `1-expansion_evaluation.jsonl`: Scored and selected expansions.
3.  `2-document_reranking.jsonl`: Documents reranked based on the query and expansion.
4.  `3-reranking_evaluation.jsonl`: Final evaluation of the reranking step (used for the recall script above).
